searchState.loadedDescShard("mistralrs_quant", 0, "Device/configurable intelligent matrix multiplication\nQuantized method for a quantized matmul.\nAdd a delta weight from LoRA to the weights. This should …\nIf the quant is backed by a qmatmul.\nBegin tracking stats into an ImatrixLayerStats\nWeight dtype and device\nEnd tracking stats into an ImatrixLayerStats. Returns the …\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nCompute matmul of <code>self</code> and <code>a</code>. <code>self</code> should contain the …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nIf the quant is backed by a qmatmul.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCompute matrix-matrix product, optionally casting to f16 …\nCompute matrix-matrix product, optionally casting to f16 …\nCompute matrix-matrix product, optionally casting to f16 …\nConvert to an equivalent gguf quantization, if applicable.\nFactor by which the weight size is reduced over the given …\nCompute quantized matrix-matrix product, optionally …\nCompute quantized matrix-matrix product, optionally …\nQuantize the model into HQQ\nIf a quantized method, return the activation dtype.")